{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 第一模块(导入包模块：必须运行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引入所有包,如果缺少某个包，包的名字附于之后\n",
    "import numpy as np\n",
    "# numpy\n",
    "\n",
    "import pandas as pd\n",
    "# pandas\n",
    "\n",
    "import requests\n",
    "# requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "# BeautifulSoup\n",
    "\n",
    "import re\n",
    "# re\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "# fake-useragent\n",
    "\n",
    "import json\n",
    "# json\n",
    "\n",
    "import time\n",
    "# time\n",
    "\n",
    "import random\n",
    "# random\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# selenium\n",
    "\n",
    "from PIL import Image,ImageEnhance\n",
    "# PIL\n",
    "\n",
    "import hashlib\n",
    "# hashlib\n",
    "\n",
    "from collections import Counter\n",
    "# collections\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "# gensim \n",
    "\n",
    "import codecs, sys\n",
    "# codecs\n",
    "\n",
    "import os\n",
    "# os\n",
    "\n",
    "import time\n",
    "#time\n",
    "\n",
    "import shutil\n",
    "# shutil\n",
    "\n",
    "import jieba\n",
    "# jieba\n",
    "\n",
    "from pyhanlp import *\n",
    "# pyhanlp，注意hanlp需要java的工具\n",
    "\n",
    "import jpype\n",
    "# jpype\n",
    "\n",
    "import tensorflow as tf\n",
    "# tensorflow\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# sklearn\n",
    "\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfparser import PDFParser, PDFDocument\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "# pdfminer3k\n",
    "\n",
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "# tk文件导入模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 第二模块(文件处理模块：必须运行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_table(root):\n",
    "    \"\"\"\n",
    "    return table:选中的表格\n",
    "    \"\"\"\n",
    "    try:\n",
    "        root.filename = filedialog.askopenfilename(filetypes=((\"xlsx\", \"*.xlsx\"),(\"xlsx\", \"*.xlsx\")))        \n",
    "        if \".xlsx\" in root.filename:\n",
    "            ### 该目录下有该文件\n",
    "            table = pd.read_excel(root.filename)\n",
    "            root.destroy()\n",
    "            return table\n",
    "    except Exception as e:\n",
    "        root.destroy()\n",
    "        print(\"导入错误\")\n",
    "\n",
    "def remote_select():\n",
    "    print(\"以下文件可以调用，需要分析哪个文件？\")\n",
    "    print(\"-------------------------------------\")\n",
    "    index = 0\n",
    "    list_item_temp = []\n",
    "    for item in os.listdir():\n",
    "        if \".xlsx\" in item:\n",
    "            index += 1\n",
    "            print(\"[\" + str(index) + \"] \" + item)\n",
    "            list_item_temp.append(item)\n",
    "    try:\n",
    "        bash_pos = \"/Users/dfuser/Desktop/目标文书目录/\"\n",
    "        file_code = int(input(\"需要导入哪个文件？(输入[]中的序号)\"))\n",
    "        final_pos = bash_pos + str(list_item_temp[file_code-1])\n",
    "        table_ = pd.read_excel(final_pos)\n",
    "        return table_\n",
    "        print(\"表格导入成功，以下是表格预览\")\n",
    "        print(\"----------------------------\")\n",
    "    except Exception as e:\n",
    "        print(\"导入错误\")        \n",
    "\n",
    "def save_model(table):\n",
    "    \"\"\"\n",
    "    Param table:需要导出的表\n",
    "    \"\"\"\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"以下为表格的预览:\")\n",
    "    print(table.head())\n",
    "    flag = input(\"是否需要保存该摘要表？(输入Y/N):\")\n",
    "    try:\n",
    "        if flag == \"Y\":\n",
    "            save_file_name = input(\"请输入该词频表的名称(不用加.xlsx):\")\n",
    "            save_file_name_xlsx = save_file_name + \".xlsx\" #导出excel文件\n",
    "            table.to_excel(save_file_name_xlsx)\n",
    "        else:\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 第三模块(核心分类器模块：必须运行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tf-idf建模后K-means聚类\n",
    "def clean(single_para,stopwords_clean):\n",
    "    \"\"\"\n",
    "    将stopwords和每一段文本取差集，清洗数据\n",
    "    param single_para:没清洗文本集中的每一段数据\n",
    "    return list_clean:每一段清洗好的文本\n",
    "    \"\"\"\n",
    "    list_clean = set(single_para).difference(set(stopwords_clean))\n",
    "    return list_clean\n",
    "\n",
    "def data_prepare(text_list):\n",
    "    \"\"\"\n",
    "    把数据清理干净\n",
    "    param text_list:没清洗过的文本列表\n",
    "    return list_clean:清洗过的文本列表\n",
    "    \"\"\"\n",
    "    table_segments_list = list(map(lambda x:jieba.lcut(x),text_list))\n",
    "    stopwords = list(map(lambda x:x.strip(\"\\n\"),codecs.open('stopwords.txt', 'r', 'utf-8').readlines()))\n",
    "    stopwords_clean = list(map(lambda x:x.strip(\"\\r\"),stopwords))\n",
    "    list_clean = list(map(clean,table_segments_list,stopwords_clean))\n",
    "    return list_clean\n",
    "\n",
    "def tf_idf_scikit(words):\n",
    "    \"\"\"\n",
    "    使用scikit-learn建立tf-idf模型\n",
    "    param words:清洗过的文本列表\n",
    "    return tf-idf:tf-idf词袋模型\n",
    "    \"\"\"\n",
    "    # 将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # 统计每个词语的tf-idf权值\n",
    "    transformer = TfidfTransformer()\n",
    "\n",
    "    # 第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵\n",
    "    tfidf = transformer.fit_transform(vectorizer.fit_transform(words))\n",
    "\n",
    "    # 获取词袋模型中的所有词语\n",
    "    word = vectorizer.get_feature_names()\n",
    "\n",
    "    # 将tf-idf矩阵抽取出来，元素w[i][j]表示j词在i类文本中的tf-idf权重\n",
    "    weight = tfidf.toarray()\n",
    "    return weight\n",
    "\n",
    "def classifier(weight,clusters_num):\n",
    "    \"\"\"\n",
    "    使用\n",
    "    param weight:tf-idf的权重\n",
    "    param clusters_num:分类簇心数量\n",
    "    return list_classification:文本与分类结果列表\n",
    "    \"\"\"\n",
    "    list_classification = []\n",
    "    ## 设计随机种子\n",
    "    random_status = random.randint(1,1000)\n",
    "\n",
    "    ## 建立Kmeans分类器\n",
    "    clf = KMeans(init='k-means++',n_clusters = clusters_num,random_state = random_status,n_init=10)\n",
    "\n",
    "    ## PCA降维处理\n",
    "    reduced_data = pca(weight)\n",
    "\n",
    "    ## 训练\n",
    "    s = clf.fit(reduced_data)\n",
    "    \n",
    "    ## 参数显示\n",
    "    # print(np.shape(clf.cluster_centers_)) #簇心\n",
    "    print(\"标签数量\",clf.labels_) \n",
    "    print(\"簇心之间的平均欧氏距离\",clf.inertia_) #簇心之间的欧氏距离\n",
    "\n",
    "    ## 文章归类处理\n",
    "    i = 1\n",
    "    while i <= len(clf.labels_):\n",
    "        list_classification.append(clf.labels_[i-1])\n",
    "        i = i + 1\n",
    "    return list_classification,clf\n",
    "\n",
    "def pca(weights):\n",
    "    reduced_data = PCA(n_components=2).fit_transform(weights)\n",
    "    return reduced_data\n",
    "\n",
    "def ploting(reduced_data,clf):\n",
    "    h = .02\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=plt.cm.Paired,\n",
    "               aspect='auto', origin='lower')\n",
    "\n",
    "    plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "\n",
    "    # Plot the centroids as a white X\n",
    "    centroids = clf.cluster_centers_\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=169, linewidths=3,\n",
    "                color='w', zorder=10)\n",
    "    plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n'\n",
    "              'Centroids are marked with white cross')\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.show()\n",
    "\n",
    "## 得到判决表\n",
    "def data_input(table):\n",
    "    \"\"\"\n",
    "    引入文本材料\n",
    "    Param table:目标列表\n",
    "    return table_judgements:含有目标文本的list\n",
    "    \"\"\"\n",
    "    table_judgements = table.dropna()\n",
    "    return table_judgements\n",
    "\n",
    "def extract_something(pattern,elem):\n",
    "    try:\n",
    "        extract_infos = re.search(pattern_judgements,elem).group()\n",
    "        return extract_infos\n",
    "    except Exception as e:\n",
    "        return \"None\"\n",
    "    \n",
    "def extract_PanJueRuXia(corpus):\n",
    "    # 判决如下\n",
    "    pattern_judgements = re.compile(\"判决如下:(.*?)。\")\n",
    "    corpus_judgements = list(map(extract_something,len(corpus) * [pattern_judgements],corpus))\n",
    "    return corpus_judgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 运行模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # ------ 导入函数开始\n",
    "    flag_input = int(input(\"你是在本机上操作？还是远程操作？(1:本机操作,2:远程操作)\"))\n",
    "    if flag_input == 1:\n",
    "        root = Tk() # 实例化TKinter窗口\n",
    "        root.withdraw() # 隐藏TKinter窗口\n",
    "        table_ready_to_eat = import_table(root)\n",
    "    elif flag_input == 2:\n",
    "        table_ready_to_eat = remote_select()\n",
    "    else:\n",
    "        logging.error(\"加载错误\")\n",
    "    # ------ 导入函数结束\n",
    "    \n",
    "    print(\"表格中所有的列:\",table_ready_to_eat.columns)\n",
    "    selected_column = input(\"请问需要分析哪一列的关键词词频？(输入关键词部分):\")\n",
    "    print(selected_column)\n",
    "    corpus = table_ready_to_eat[selected_column]\n",
    "    \n",
    "    table_judgements = data_input(pd.Series(corpus))\n",
    "    data_test = data_prepare(table_judgements)\n",
    "\n",
    "    ## 单词列表组成字符串\n",
    "    words = list(map(lambda x:\" \".join(list(x)),data_test))\n",
    "\n",
    "    ## tf-idf转换weight\n",
    "    weight = tf_idf_scikit(words)\n",
    "\n",
    "    ## 请输入主题分类数量\n",
    "    num_topics = int(input(\"请输入需要分类的主题数量:\"))\n",
    "    \n",
    "    ## 分类器得到分类\n",
    "    list_classification,clf = classifier(weight,num_topics)\n",
    "\n",
    "    ## pca降维\n",
    "    reduced_data = pca(weight)\n",
    "\n",
    "    ## 将分类情况可视化\n",
    "    ploting(reduced_data,clf)\n",
    "\n",
    "    ## 将分类结果形成列表\n",
    "    table_kmeans_result = pd.DataFrame([list(table_judgements),list_classification]).T\n",
    "    \n",
    "    ## 存储该表\n",
    "    save_model(table_kmeans_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
